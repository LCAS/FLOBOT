<!DOCTYPE html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>FLOBOT Perception Dataset</title>
    <link rel="stylesheet" href="css/wp.css" type="text/css" />
  </head>
  
  <body>
    <h1 id="logo">
      <a href="http://www.flobot.eu/"><img alt="flobot_logo.png" src="images/flobot_logo.png"/></a>&nbsp;&nbsp;&nbsp;
      <a href="https://www.tuwien.ac.at/"><img alt="tuw_logo.png" src="images/tuw_logo.png"/></a>&nbsp;&nbsp;&nbsp;
      <a href="https://www.lincoln.ac.uk/"><img alt="uol_logo.png" src="images/uol_logo.png"/></a>&nbsp;&nbsp;&nbsp;
      <a href="https://lcas.lincoln.ac.uk/wp/"><img alt="lcas_logo.png" src="images/lcas_logo.png"/></a>      
      <br/>FLOBOT Perception Dataset
    </h1>
    
    <p style="text-align: left">Collected by our very own FLOBOT (FLOor washing RObot)</p>
    <p style="text-align: left"><img alt="flobot.jpg" height="315" src="images/flobot.jpg"></p>
    
    <h2>Description</h2>
    <p>This dataset was collected with FLOBOT - an advanced autonomous floor scrubber - includes data from four different sensors for environment perception, as well as the robot pose in the world reference frame.
      Specifically, FLOBOT relies on a 3D lidar and a RGB-D camera for human detection and tracking, and a second RGB-D and a stereo camera for dirt and object detection.
      Data collection was performed in four public places (three of them are released in this dataset), two in Italy and two in France, in FOLBOT working mode with the corresponding testing procedures for final project validation.
      For a quick overview, please refer to the following video.</p>
    <div style="text-align: left"><iframe width="560" height="315" src="https://www.youtube.com/embed/mbilkt5tfoo" frameborder="0" allowfullscreen></iframe></div>
    
    <h2>Contributions</h2>
    <ol>
      <li><a href="http://wiki.ros.org/">Robot Operating System (ROS)</a> <i>rosbag</i> files recorded from four sensors including a 3D lidar, two RGB-D cameras and a stereo camera, and the robot pose in the world reference frame are provided. All the sensory data are synchronized at the software level (i.e. time stamped by ROS).</li>
      <li>Data collection was carried out with the real FLOBOT prototype, in real environments including airport, warehouse and supermarket. While these public places are rarely to obtain permission to perform data collection especially with robots.</li>
      <li>Annotation of pedestrian in 3D lidar, dirt and object in RGB-D camera are provided.</li>
      <li>Although not our main use, since the dataset provides also robot pose in the world reference frame, it can be used for localization and mapping problems. Moreover, as our data involves very characteristic public scenarios (i.e. airport, warehouse and supermarket), it is also suitable for semantic and contextual study.</li>
    </ol>
    
    <h2>Citation</h2>
    <p>If you publish work based on, or using, this dataset, we would appreciate citations to the following:</p>
    <p><b>manuscript in preparation ...</b></p>
    
    <h2>Recording platform</h2>
    <table>
      <tr>
	<td><img alt="flobot_sensors.jpg" height="315" src="images/flobot_sensors.jpg"></td>
	<td>
	  <ol>
	    <li>Velodyne VLP-16 3D lidar
	    <li>Xtion PRO LIVE RGB-D camera (forward-facing for human detection)
	    <li>Xtion PRO LIVE RGB-D camera (floor-facing for dirt and object detection)
	    <li>ZED stereo camera (floor-facing for dirt and object detection)
	    <li>SICK S300 2D lidar
	    <li>OEM incremental measuring wheel encoder
	    <li>Xsens MTi-30 IMU (inside of the robot)
          </ol>
	</td>
      </tr>
    </table> 
    
    <h2>Recording environments</h2>
    <p>
      Four pilot sites were selected for the final FLOBOT validation, which led to this dataset (with three of them).
      These pilot sites descriptions are important in order to understand the requirements for each use case and accordingly design the FLOBOT robot and complete system.
    </p>
    <table style="border-spacing: 20px; text-align: center;">
      <tr>
	<td><img alt="airport.jpg" height="168" src="images/airport.jpg"></td>
	<td><img alt="warehouse.jpg" height="168" src="images/warehouse.jpg"></td>
	<td><img alt="supermarket.jpg" height="168" src="images/supermarket.jpg"></td>
      </tr>
      <tr>
	<td>Airport</td>
	<td>Warehouse</td>
	<td>Supermarket</td>
      </tr>
    </table>
    
    <h2>Downloads</h2>
    
    <p style="text-align: left">
      <img alt="dataset_locations.jpg" height="315" src="images/dataset_locations.jpg">
    </p>

    <table style="border-spacing: 20px; text-align: left;">
      <tr> <th>Date</th> <th>Time (GMT+2)</th> <th>Place (Europe)</th> <th>Sensors</th> <th>Main purposes</th> <th>Downloads</th> </tr>
      <tr> <td>2018-04-19</td> <td>11:41-11:49 (8:24s)</td> <td>Carugate (supermarket)</td> <td>Velodyne</td> <td>Human tracking</td> <td>supermarket-2018-04-19-11-41-21-velodyne-only.bag</td> </tr>
      <tr> <td>2018-05-31</td> <td>16:35-16:39 (3:44s)</td> <td>Carugate (supermarket)</td> <td>Velodyne / Xtion (depth)</td> <td>Human tracking</td> <td>supermarket-2018-05-31-16-35-33.bag</td> </tr>
      <tr> <td>2018-06-12</td> <td>17:10-17:13 (3:27s)</td> <td>Lyon (warehouse)</td> <td>Velodyne / Xtion (depth)</td> <td>Human tracking</td> <td>warehouse-2018-06-12-17-10-22.bag</td> </tr>
      <tr> <td>2018-06-13</td> <td>16:11-16:17 (5:05s)</td> <td>Lyon (airport)</td> <td>Velodyne / Xtion (depth)</td> <td>Human tracking</td> <td>airport-2018-06-13-16-11-56.bag</td> </tr>
      <tr> <td>2018-06-13</td> <td>16:20-16:23 (2:26s)</td> <td>Lyon (airport)</td> <td>Velodyne / Xtion (depth)</td> <td>Human tracking</td> <td>airport-2018-06-13-16-20-34.bag</td> </tr>
      <tr> <td>2018-06-13</td> <td>16:37-16:42 (4:28s)</td> <td>Lyon (airport)</td> <td>Velodyne / Xtion (depth)</td> <td>Human tracking</td> <td>airport-2018-06-13-16-37-32.bag</td> </tr>
    </table>
    
    <h2>How to play</h2>
    
    <p><i>roslaunch <a href="https://lcas.lincoln.ac.uk/owncloud/index.php/s/7en7ugoKy5YPZxA">flobot_dataset_play.launch</a> bag:=path_to_your_rosbag</i> (<a href="https://lcas.lincoln.ac.uk/owncloud/index.php/s/qlgMApVrGFWdPUo">rviz config here</a>ï¼‰</p>

    <h2>Open source</h2>
    <ul>
      <li>Human detection and tracking: <a href="https://github.com/LCAS/FLOBOT">https://github.com/LCAS/FLOBOT</a></li>
      <li>Object and dirt detection:</li>
    </ul>

    <h2>Related publications</h2>
    <ol>
      <li>Zhi Yan, Tom Duckett, and Nicola Bellotto. <b>Online learning for 3D LiDAR-based human detection: Experimental analysis of point cloud clustering and classification methods</b>. <i> Autonomous Robots</i>, 2019. [<a href="https://yzrobot.github.io/publications/yz_bib.html#yz19auro">BibTeX</a> | <a href="https://rdcu.be/bODuU">PDF</a>]</li>
      <li>Georg Halmetschlaeger-Funek, Markus Suchi, Martin Kampel, and Markus Vincze. <b>An empirical evaluation of ten depth cameras: Bias, precision, lateral noise, different lighting conditions and materials, and multiple sensor setups in indoor environments</b>. In <i>IEEE Robotics & Automation Magazine</i>, 2019. [<a href="https://ieeexplore.ieee.org/document/8436009">PDF</a> | <a href="https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/rgbd-sensor-tests/">Dataset</a>]</li>
      <li>Zhi Yan, Li Sun, Tom Duckett, and Nicola Bellotto. <b> Multisensor online transfer learning for 3D LiDAR-based human detection with a mobile robot</b>. In <i>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, Madrid, Spain, October 2018. [<a href="https://yzrobot.github.io/publications/yz_bib.html#yz18iros">BibTeX</a> | <a href="https://arxiv.org/abs/1801.04137">PDF</a> | <a href="https://github.com/LCAS/online_learning/tree/multisensor">Code</a> | <a href="https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-multisensor-people-dataset/">Dataset</a>]</li>
      <li>Georg Halmetschlaeger-Funek, Johann Prankl, and Markus Vincze. <b>Towards autonomous auto calibration of unregistered RGB-D setups: The benefit of plane priors</b>. In <i>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, Madrid, Spain, October 2018. [<a href="https://ieeexplore.ieee.org/document/8593715">PDF</a>]</li>
      <li>Li Sun, Zhi Yan, Sergi Molina Mellado, Marc Hanheide, and Tom Duckett. <b>3DOF pedestrian trajectory prediction learned from long-term autonomous mobile robot deployment data</b>. In <i>Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA)</i>, Brisbane, Australia, May 2018. [<a href="https://yzrobot.github.io/publications/yz_bib.html#ls18icra">BibTeX</a> | <a href="https://arxiv.org/abs/1710.00126">PDF</a> | <a href="https://lcas.lincoln.ac.uk/wp/3dof-pedestrian-trajectory-dataset/">Dataset</a> | <a href="https://youtu.be/STmc_g7PwtE">Video</a>]</li>
      <li>Farhoud Malekghasemi, Georg Halmetschlaeger-Funek, and Markus Vincze. <b>Autonomous extrinsic calibration of a depth sensing camera on mobile robots</b>. In <i>Proceedings of the Austrian Robotics Workshop (ARW)</i>, Innsbruck, Austria, May 2018 [<a href="https://www.researchgate.net/publication/326635998_Autonomous_Extrinsic_Calibration_of_a_Depth_Sensing_Camera_on_Mobile_Robots">PDF</a>]</li>
      <li>Zhi Yan, Tom Duckett, and Nicola Bellotto. <b>Online learning for human classification in 3D LiDAR-based tracking</b>. In <i>Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, pages 864-871, Vancouver, Canada, September 2017. [<a href="https://yzrobot.github.io/publications/yz_bib.html#yz17iros">BibTeX</a> | <a href="https://yzrobot.github.io/publications/yz17iros.pdf">PDF</a> | <a href="https://github.com/LCAS/online_learning">Code</a> | <a href="https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/">Dataset</a> | <a href="https://youtu.be/bjztHV9rC-0">Video1</a> | <a href="https://youtu.be/rmPn7mWssto">Video2</a>]</li>
      <li>Andreas Gr&uuml;nauer, Georg Halmetschlaeger-Funek, Johann Prankl, and Markus Vincze. <b>The power of GMMs: Unsupervised dirt spot detection for industrial floor cleaning robots</b>. In <i>Proceedings of the Towards Autonomous Robotic Systems - 18th Annual Conference (TAROS)</i>, Guildford, UK, July 2017. [<a href="https://www.researchgate.net/publication/318539021_The_Power_of_GMMs_Unsupervised_Dirt_Spot_Detection_for_Industrial_Floor_Cleaning_Robots">PDF</a> | <a href="https://owncloud.tuwien.ac.at/index.php/s/4eydrOcCJrgwBFQ">Dataset</a>]</li>
      <li>Simon Schreiberhuber, Thomas M&ouml;rwald, and Markus Vincze. <b>Bilateral filters for quick 2.5D plane segmentation</b>. In <i>Proceedings of the OAGM&ARW Joint Workshop</i>, Vienna, Austria, May 2017. [<a href="https://diglib.tugraz.at/download.php?id=5aaa459331014&location=browse">PDF</a>]</li>
    </ol> 
    
    <h2>License</h2>
    <p>
      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
      <br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
      <br />Copyright (c) 2019 Simon Schreiberhuber, Georg Halmetschlager, Markus Vincze, Zhi Yan, Tom Duckett, and Nicola Bellotto.
    </p>
    
    <h2>Funding</h2>
    <p>
      <img alt="euflag.png" height="80" src="images/euflag.png"/>
      <br />This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 645376 (<a href="http://www.flobot.eu/">FLOBOT</a>).
    </p>
  </body>
</html>
